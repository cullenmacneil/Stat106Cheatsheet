\documentclass[10pt, landscape]{article}
\usepackage[margin=0.25in]{geometry}
\usepackage{multicol}
\usepackage{amsmath, amssymb}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{graphicx}

% Cheatsheet styling
\titleformat{\section}{\large\bfseries}{}{0em}{}
\titlespacing{\section}{0pt}{*1}{*0.5}
\titleformat{\subsection}{\normalsize\bfseries}{}{0em}{}
\titlespacing{\subsection}{0pt}{*0.8}{*0.2}

\pagestyle{fancy}
\fancyhf{}
\rfoot{Page \thepage}
\setlength{\columnsep}{0.5cm}

\begin{document}
\begin{multicols}{3}

\section{Stat 106}
\subsection{Cullen MacNeil}

\section{General Analytic \& Sports Metrics}
\subsection{Expected Points (EP)}
\begin{itemize}[noitemsep]
    \item Average points scored given a specific situation or state in a game.
    \item Calculated based on historical play-by-play data.
    \item Example: Football EP at given yard line and down.
\end{itemize}

\subsection{Expected Points Added (EPA)}
\begin{itemize}[noitemsep]
    \item Measure of value added by a play relative to expectation.
    \item Formula:
    \[ EPA = Points - EP \]
    \item Interpretation: Positive EPA indicates better-than-expected outcomes.
\end{itemize}

\subsection{Win Probability (WP) \& Win Probability Added (WPA)}
\begin{itemize}[noitemsep]
    \item \textbf{Win Probability (WP)}: Likelihood of winning given the current game state.
    \item \textbf{Win Probability Added (WPA)}: Change in WP from before to after a specific play.
    \item Formula:
    \[ WPA = WP_{\text{after}} - WP_{\text{before}} \]
    \item High leverage situations (e.g., late-game) significantly affect WPA.
\end{itemize}

\section{Core Analytical Ideas}

\subsection{Stickiness, Leverage, Clutch-ness}
\begin{itemize}[noitemsep]
    \item \textbf{Stickiness}: Stability of performance metrics over time.
    \item \textbf{Leverage}: Situational importance; high-leverage moments significantly influence outcomes.
    \item \textbf{Clutch-ness}: Ability to perform well in high-leverage situations.
\end{itemize}

\subsection{Luck \& Mean Reversion}
\begin{itemize}[noitemsep]
    \item \textbf{Luck}: Random deviations from expected performance metrics.
    \item \textbf{Mean Reversion}: Tendency for extreme performance to return toward average levels over time.
\end{itemize}

\subsection{Shrinkage Estimates}
\begin{itemize}[noitemsep]
    \item Estimates adjusted towards a prior or mean to reduce variance.
    \item Useful for stabilizing performance estimates, particularly with limited data.
    \item Prevents overfitting and extreme predictions.
\end{itemize}

\section{Regression Models}

\subsection{Linear Regression}
\begin{itemize}[noitemsep]
    \item Model form: \( Y = \beta_0 + \beta_1 X + \epsilon \).
    \item Interpretation: Coefficient \(\beta_1\) is the average change in \(Y\) per unit change in \(X\).
    \item Assumptions: linearity, independence, homoscedasticity, normality.
\end{itemize}

\subsection{Logistic Regression}
\begin{itemize}[noitemsep]
    \item Used for binary outcomes; models log-odds of an event.
    \item Model form: \( \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X \).
    \item Coefficients represent log-odds; \(e^{\beta}\) gives odds ratios.
\end{itemize}

\subsection{Linear vs Logistic}
\begin{itemize}[noitemsep]
    \item \textbf{Linear}: Continuous response variable.
    \item \textbf{Logistic}: Binary response variable.
    \item Choose based on outcome type (continuous vs. categorical).
\end{itemize}

\section{Transformations \& Interactions}

\subsection{Log Transformations}
\begin{itemize}[noitemsep]
    \item Useful to stabilize variance and normalize skewed data.
    \item Often used when relationships between variables are multiplicative.
\end{itemize}

\subsection{Polynomial Transformations}
\begin{itemize}[noitemsep]
    \item Capture non-linear relationships using powers of predictors (e.g., quadratic, cubic).
    \item Example: \( Y = \beta_0 + \beta_1X + \beta_2X^2 + \epsilon \).
\end{itemize}

\subsection{Interaction Effects}
\begin{itemize}[noitemsep]
    \item Effect of one predictor on the response depends on another predictor.
    \item Modeled by multiplying two predictors: \( Y = \beta_0 + \beta_1X + \beta_2Z + \beta_3(XZ) + \epsilon \).
\end{itemize}

\section{Simulation \& Resampling}

\subsection{Bootstrapping}
\begin{itemize}[noitemsep]
    \item Repeated resampling from observed data to estimate sampling distribution.
    \item Useful for confidence intervals, bias estimation, and variance reduction.
\end{itemize}

\subsection{Permutation Testing}
\begin{itemize}[noitemsep]
    \item Hypothesis testing by reshuffling labels to determine significance.
    \item No assumptions about underlying distribution required.
\end{itemize}

\section{Rating \& Ranking Systems}

\subsection{Bradley-Terry Models}
\begin{itemize}[noitemsep]
    \item Models probability of winning pairwise comparisons.
    \item Probability team \(A\) beats team \(B\):
    \[ P(A \text{ beats } B) = \frac{e^{\beta_A}}{e^{\beta_A} + e^{\beta_B}} \]
    \item Estimates team strength parameters \(\beta\) via logistic regression framework.
\end{itemize}

\subsection{Bradley-Terry (Quantitative)}
\begin{itemize}[noitemsep]
    \item Extends Bradley-Terry model to predict numeric outcomes.
    \item Suitable for scores or other continuous metrics instead of binary win-loss outcomes.
\end{itemize}

\subsection{Elo Models}
\begin{itemize}[noitemsep]
    \item Dynamic rating system updating ratings based on outcomes.
    \item Win Probability formula:
    \[ P = \frac{1}{1 + 10^{(R_B - R_A)/400}} \]
    \item Update formula:
    \[ R_{\text{new}} = R_{\text{old}} + k(\text{Outcome} - \text{Expected}) \]
    \item Parameter \(k\) adjusts sensitivity to new results.
\end{itemize}

\subsection{KenPom Efficiency}
\begin{itemize}[noitemsep]
    \item Basketball-specific metrics evaluating team efficiency.
    \item Offensive and defensive ratings adjusted for opponent strength and pace.
    \item Higher efficiency indicates stronger overall performance.
\end{itemize}

\section{Predictive Modeling \& Validation}

\subsection{Train-Test-Validation Split}
\begin{itemize}[noitemsep]
    \item Data partitioned into subsets:
    \begin{itemize}[noitemsep]
        \item \textbf{Training}: Model fitting.
        \item \textbf{Validation}: Model selection and tuning.
        \item \textbf{Test}: Evaluate out-of-sample predictive performance.
    \end{itemize}
\end{itemize}

\subsection{Cross-Validation}
\begin{itemize}[noitemsep]
    \item Technique to assess model predictive performance.
    \item Data repeatedly split into training and validation subsets.
    \item Commonly used form: k-fold CV, data split into k subsets.
\end{itemize}

\subsection{Prediction, Overfitting, Complexity}
\begin{itemize}[noitemsep]
    \item \textbf{Overfitting}: Model captures noise, poor generalization.
    \item Balance complexity (number of parameters) vs. prediction accuracy.
    \item Cross-validation helps identify appropriate model complexity.
\end{itemize}

\section{Model Selection \& Regularization}

\subsection{Sequential Variable Selection (AIC)}
\begin{itemize}[noitemsep]
    \item Iteratively adds/removes variables based on Akaike Information Criterion (AIC).
    \item AIC formula: \( AIC = 2p - 2\ln(L) \), penalizes model complexity.
\end{itemize}

\subsection{Penalized Regression}
\begin{itemize}[noitemsep]
    \item Adds penalty term to regression to prevent overfitting.
    \item \textbf{Ridge Regression}: Penalizes squared coefficients \(\sum \beta_j^2\).
    \item \textbf{LASSO Regression}: Penalizes absolute value of coefficients \(\sum |\beta_j|\), shrinks some coefficients to exactly zero.
\end{itemize}

\section{Advanced Predictive Techniques}

\subsection{Random Forests}
\begin{itemize}[noitemsep]
    \item Ensemble of decision trees built on bootstrapped samples.
    \item Reduces variance and improves prediction by averaging outcomes.
    \item Each split considers random subset of predictors.
\end{itemize}

\subsection{Hyperparameter Tuning}
\begin{itemize}[noitemsep]
    \item Process of optimizing model parameters not learned from data.
    \item Common methods: Grid Search, Random Search, Cross-Validation.
    \item Helps prevent overfitting and improves predictive performance.
\end{itemize}

\section{Additional Quick Reference}

\subsection{Regression Assumptions}
\textbf{OLS Linear Regression}:
\begin{itemize}[noitemsep]
    \item Linearity, independence, homoscedasticity, normality of residuals.
\end{itemize}

\textbf{Logistic Regression}:
\begin{itemize}[noitemsep]
    \item Independence, linearity in logit scale.
\end{itemize}

\subsection{Important Metrics \& Formulas}

\textbf{Pythagorean Wins}:
\[
E(\text{Wins}) = G \times \frac{RS^2}{RS^2 + RA^2}
\]

\textbf{Confidence Interval (mean)}:
\[
\bar{x} \pm t^*\frac{s}{\sqrt{n}}
\]

\textbf{Z-Test Statistic}:
\[
Z = \frac{\bar{x} - \mu_0}{\sigma/\sqrt{n}}
\]

\subsection{Essential R Functions}

\textbf{dplyr (Tidyverse)}:
\begin{itemize}[noitemsep]
    \item \texttt{filter(data, condition)}
    \item \texttt{select(data, columns)}
    \item \texttt{mutate(data, new\_var = expression)}
    \item \texttt{summarise(data, new\_var = summary\_function)}
\end{itemize}

\textbf{Regression and Trees}:
\begin{itemize}[noitemsep]
    \item Linear Regression: \texttt{lm(y \textasciitilde{} x, data)}
    \item Logistic Regression: \texttt{glm(y \textasciitilde{} x, data, family = "binomial")}
    \item Regression Trees: \texttt{rpart(y \textasciitilde{} x, data)}
    \item Random Forests: \texttt{randomForest(y \textasciitilde{} x, data)}
\end{itemize}

\subsection{Model Interpretation}
\begin{itemize}[noitemsep]
    \item Linear regression coefficient: change in \(Y\) per unit change in \(X\).
    \item Logistic regression coefficient: log-odds, \(e^\beta\) for odds ratio.
    \item Adjusted \(R^2\): accounts for number of predictors.
\end{itemize}

\subsection{Quick Visualization (ggplot2)}
\begin{itemize}[noitemsep]
    \item Histogram: \texttt{geom\_histogram()}
    \item QQ plot: \texttt{stat\_qq(), stat\_qq\_line()}
    \item Scatterplot: \texttt{geom\_point(), geom\_smooth(method = "lm")}
\end{itemize}


\end{multicols}
\end{document}
